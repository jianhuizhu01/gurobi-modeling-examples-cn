{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0452227",
   "metadata": {},
   "source": [
    "Copyright © 2023 Gurobi Optimization, LLC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2a1cec",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 使用线性规划计算文本差异度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a855eb7d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "在本案例中,我们将通过一个独特的示例来演示如何应用优化技术来评估文本的差异性。这种技术有很多潜在的应用,比如检测抄袭、信息检索、聚类分析、文本分类、主题检测、问答系统、机器翻译和文本摘要等。更多信息请参阅[这里](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=1aff7b429f99f529228a4299a5794971adeb1ca3#:~:text=There%20are%20several%20applications%20or,machine%20translation%2C%20text%20summarization%20etc)。\n",
    "\n",
    "**词移距离(Word Mover's Distance, WMD)** 是一种流行的文本相似度度量方法,用于衡量两份文档之间的语义距离。在本案例中,我们将实现两个目标:\n",
    "- 给定两段文本,将WMD建模为优化问题并计算它\n",
    "- 检查一本书中被抄袭的段落,然后在该书中找到与给定段落语义最接近的原始段落\n",
    "\n",
    "|<img src=\"https://raw.githubusercontent.com/Gurobi/modeling-examples/master/text_dissimilarity/figure_obama.png\" width=\"500\" align=\"center\">| \n",
    "|:--:|\n",
    "|用词移距离来衡量两份文档的相似度示意图。<b>图片来源: [Towards AI](https://towardsai.net/p/nlp/word-movers-distance-wmd-explained-an-effective-method-of-document-classification-89cb258401f4) </b>| "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1d812d",
   "metadata": {},
   "source": [
    "## I. 数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770378b5",
   "metadata": {},
   "source": [
    "### Google词向量数据\n",
    "\n",
    "为了找到词语之间的语义距离,我们首先需要为每个词构建向量嵌入。\n",
    "我们使用来自Google News的流行'word2vec'数据集。该数据集包含了300万个预训练的词向量嵌入。点击[这里](https://code.google.com/archive/p/word2vec/)了解更多关于该数据集的信息。\n",
    "\n",
    "现在我们导入所有需要的包并下载word2vec数据。注意,在下面的代码中,下载数据大约需要一分钟。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f2bb6d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt   \n",
    "from gensim.models import KeyedVectors \n",
    "from scipy import spatial\n",
    "\n",
    "import gensim.downloader as api\n",
    "model = api.load('word2vec-google-news-300')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee9fe5c",
   "metadata": {},
   "source": [
    "我们已经下载了大量的词向量嵌入数据集。例如,\"Sherlock\"这个词在300维空间中的嵌入向量如下所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42792a64",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['Sherlock']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f699b616",
   "metadata": {},
   "source": [
    "### 输入两份文档\n",
    "\n",
    "接下来,我们创建要比较的两份文档。在这个例子中,我们看一下来自阿瑟·柯南·道尔的《福尔摩斯探案集》(最早的数据驱动型侦探小说)中的一句话:\n",
    "\n",
    "\"The little man stood glancing from one to the other of us with half-frightened, half-hopeful eyes, as one who is not sure whether he is on the verge of a windfall or of a catastrophe.\"\n",
    "\n",
    "\n",
    "我们使用流行的文本生成技术([ChatGPT](https://chat.openai.com/chat))来构建一个语义相似的句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c59d76c",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Gurobi/modeling-examples/master/text_dissimilarity/chatgpt clip.gif\" width=\"750\" align=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76965342",
   "metadata": {},
   "source": [
    "我们将这两个句子(文档)存储为字符串。你也可以尝试其他句子对。一些示例在下面的注释中给出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11248557",
   "metadata": {},
   "outputs": [],
   "source": [
    "document1 = 'The little man stood glancing from one to the other of us with half-frightened, half-hopeful eyes, as one who is not sure whether he is on the verge of a windfall or of a catastrophe.'\n",
    "document2 = 'With a gaze that shifted back and forth between us, the diminutive figure appeared to be a mixture of apprehension and anticipation, uncertain if he was on the cusp of a fortune or a disaster.'\n",
    "\n",
    "# document1 = 'I barely saw Sherlock recently.'\n",
    "# document2 = 'Lately, I have had little opportunity to catch a glimpse of Sherlock.'\n",
    "\n",
    "# document1 = 'Obama speaks to the media in Illinois.'\n",
    "# document2 = 'The President greets the press in Chicago'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5560ff9",
   "metadata": {},
   "source": [
    "## II. 文本预处理\n",
    "\n",
    "在比较两个文档之前,我们首先要删除那些可能不携带任何语义价值的词,例如介词(如in、on、under)、连词(如and、for、but)和限定词(如a、an、the、another)。这些词被称为**停用词**。更多信息请参见[这里](https://kavita-ganesan.com/what-are-stop-words/#.ZCCurezMLt0)。\n",
    "我们还要删除标点符号和专有名词,然后将所有单词转换为小写形式。\n",
    "\n",
    "我们使用nltk包来实现这一点。在下面的代码中,包含\"nltk.download\"的行需要在第一次运行这个notebook时执行。后续运行时可以注释掉这些行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcc77a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger_eng')  \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer, word_tokenize\n",
    "from nltk.stem import PorterStemmer \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3460d5",
   "metadata": {},
   "source": [
    "将每个文档拆分成单词(或\"标记\")。每个单词被分类到几个类别中。例如,'NN'对应单数名词,'VBD'是过去时态动词。完整的分类列表请参见[这里](https://pythonprogramming.net/natural-language-toolkit-nltk-part-speech-tagging/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a89731ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tagged_doc1 = nltk.tag.pos_tag(document1.split())\n",
    "tagged_doc2 = nltk.tag.pos_tag(document2.split()) \n",
    "print(tagged_doc1)\n",
    "print(tagged_doc2) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc1d6d",
   "metadata": {},
   "source": [
    "了解了这些类别后,我们首先移除专有名词,即分类为NNP(单数专有名词)和NNPS(复数专有名词)的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddabb840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "edited_sentence1 = [word for word,tag in tagged_doc1 if tag not in ['NNP','NNPS']]\n",
    "edited_sentence2 = [word for word,tag in tagged_doc2 if tag not in ['NNP','NNPS']] \n",
    "print(edited_sentence1)\n",
    "print(edited_sentence2) \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6265df6f",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "接下来,我们删除所有的标点符号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5e7611c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+') \n",
    "processed_doc1 = tokenizer.tokenize(' '.join(edited_sentence1))\n",
    "processed_doc2 = tokenizer.tokenize(' '.join(edited_sentence2))\n",
    "print(processed_doc1)\n",
    "print(processed_doc2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39b43c8",
   "metadata": {},
   "source": [
    "现在我们删除所有的停用词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d7040484",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "processed_doc1 = [x.lower() for x in processed_doc1]\n",
    "processed_doc2 = [x.lower() for x in processed_doc2]\n",
    "processed_doc1 = [i for i in processed_doc1 if i not in stopwords.words('english')]\n",
    "processed_doc2 = [i for i in processed_doc2 if i not in stopwords.words('english')]\n",
    "\n",
    "print(processed_doc1,'\\n', processed_doc2) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b58f6",
   "metadata": {},
   "source": [
    "为了更好地可视化处理后的文档,让我们创建它们的词云图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "550dc930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "print(\"Document 1:\") \n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join(processed_doc1))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Document 2:\")\n",
    "wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(' '.join(processed_doc2))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c977b8ef",
   "metadata": {},
   "source": [
    "为了准备优化模型的文档,让我们存储每个词的出现频率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3f9c093",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "freqency_D1 = {i: processed_doc1.count(i)/len(processed_doc1) for i in processed_doc1}\n",
    "freqency_D2 = {i: processed_doc2.count(i)/len(processed_doc2) for i in processed_doc2}\n",
    "\n",
    "D1 = set(processed_doc1)\n",
    "D2 = set(processed_doc2) \n",
    "print(D1,D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cf37a4",
   "metadata": {},
   "source": [
    "现在我们得到词对词的距离矩阵。给定两个词的向量嵌入$\\overline{x}_1$和$\\overline{x}_2$,我们取它们之间的余弦距离,即$\\overline{x}_1 . \\overline{x}_2$/$|\\overline{x}_1||\\overline{x}_2|$。取消下面代码的注释可以看到两个文档中所有词对之间的排序距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "547ce764",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "distance = {(i,j): spatial.distance.cosine(model[i],model[j]) for i in D1 for j in D2} \n",
    "\n",
    "# dict(sorted(distance.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898014e1",
   "metadata": {},
   "source": [
    "## III. 使用线性规划计算文本差异度\n",
    "\n",
    "现在,我们将词移距离(WMD)建模为优化问题。\n",
    "基本思想是从文档1中的词向文档2中的词发送*流量*,使得距离与流量的乘积最小化。\n",
    "换句话说,我们在语义上更接近的词对之间发送更大的流量。\n",
    "\n",
    "在下图中,我们可以看到最优流量对应于语义上最接近的词对。\n",
    "\n",
    "|<img src=\"https://raw.githubusercontent.com/Gurobi/modeling-examples/master/text_dissimilarity/figure_obama2.png\" width=\"500\" align=\"center\">| \n",
    "|:--:|\n",
    "|用词移距离来衡量两份文档的相似度示意图。<b>图片来源: [Towards AI](https://towardsai.net/p/nlp/word-movers-distance-wmd-explained-an-effective-method-of-document-classification-89cb258401f4) </b>| \n",
    "\n",
    "<!-- 得分:距离的加权和。 -->\n",
    "\n",
    "在优化术语中,这个模型被称为**运输模型**,其中流量对应于在不同位置之间运输商品(如仓库和零售点之间)。\n",
    "\n",
    "\n",
    "在构建模型之前,我们先定义输入参数。\n",
    "\n",
    "### 输入参数\n",
    "\n",
    "$D_1, D_2$: 两个文档,每个文档代表一组词,\n",
    "\n",
    "$p_w$: 词$w$在$D_1$中的频率,\n",
    "\n",
    "$q_{w'}$: 词$w'$在$D_2$中的频率,\n",
    "\n",
    "$d(w,w')$: 词$w$和$w'$的词向量嵌入之间的距离。\n",
    "\n",
    "要构建模型,我们首先初始化Gurobi模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b72b117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "%pip install gurobipy\n",
    "import gurobipy as gp\n",
    "from gurobipy import GRB\n",
    "\n",
    "# 初始化模型\n",
    "m = gp.Model(\"Text_similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9531ec3",
   "metadata": {},
   "source": [
    "### 流量变量\n",
    "\n",
    "以下是模型中的关键决策变量。\n",
    "\n",
    "$f_{w,w'}$: 从$D_1$中的词$w$到$D_2$中的词$w'$的流量。\n",
    "\n",
    "\n",
    "\n",
    "我们可以使用addVars函数为所有词对添加流量变量。我们允许流量变量在$0$和$1$之间,因此将下界(lb)设为$0$,上界(ub)设为$1$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0144817",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = m.addVars(D1,D2,name=\"f\",lb=0,ub=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b527a3f",
   "metadata": {},
   "source": [
    "### 目标函数:最小化距离与流量的乘积总和\n",
    "\n",
    "\n",
    "流量越大,这些词在语义上应该越接近。这是通过定义目标函数为距离和流量的乘积来实现的,用数学表达式为:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\textrm{minimize} \\ \\sum_{w \\in D1} \\sum_{w' \\in D2}&  d(w,w') f_{w,w'}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "\n",
    "最小化这个目标函数自然会为距离较小的词对分配较高的流量值。我们可以在下面将这个目标函数添加到Gurobi模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dbcdfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "m.ModelSense = GRB.MINIMIZE\n",
    "m.setObjective(sum(f[w,w_prime]*distance[w,w_prime] for w in D1 for w_prime in D2))#/sum(cost[w,w_prime] for w in D for w_prime in D_prime))\n",
    "m.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d2a9ba",
   "metadata": {},
   "source": [
    "### 约束条件\n",
    "\n",
    "\n",
    "最后,我们定义约束条件来确保每个词在流量中的表示与其出现频率成比例。\n",
    "我们通过确保从文档$D1$中每个词$w$流出的净流量等于它在$D1$中的出现频率来实现这一点。\n",
    "同样,流入文档$D2$中每个词$w'$的净流量等于它在$D2$中的出现频率。\n",
    "这两个约束条件可以用下面的两个方程表示。\n",
    "\n",
    "\\begin{aligned}\n",
    " \\sum_{w' \\in D2} f_{w,w'} &= p_w \\quad  \\forall \\ w \\in D1, \\\\\n",
    "\\ \\sum_{w \\in D1} f_{w,w'} &= q_{w'} \\quad  \\forall  \\ w' \\in D2\n",
    "\\end{aligned}\n",
    "\n",
    "我们可以使用addConstrs函数将这些约束条件添加到Gurobi模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba210f",
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.addConstrs(f.sum(w, '*') == freqency_D1[w] for w in D1)\n",
    "m.addConstrs(f.sum('*', w_prime) == freqency_D2[w_prime] for w_prime in D2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29514d5c",
   "metadata": {},
   "source": [
    "### 求解模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ad9c8",
   "metadata": {},
   "source": [
    "我们已经添加了所有的决策变量、目标函数和约束条件,现在可以求解模型了!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f35da347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "m.optimize()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237b9dc0",
   "metadata": {},
   "source": [
    "模型已求解,流量解被存储在Pandas数据框中以便于可视化。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deb8620",
   "metadata": {},
   "source": [
    "我们如何解释这个结果?多大的差异度才足以检测抄袭?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e880771",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {},
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Dissimilarity score:\",round((m.ObjVal),2),\"\\n\")\n",
    "\n",
    "solution = pd.DataFrame()\n",
    "flow = {(i,j): f[i,j].X for i in D1 for j in D2 if f[i,j].X > 0} \n",
    "# flow = sorted(flow.items(), key=lambda item: item[1],reverse=True)\n",
    "solution['word 1'] = [i for (i,j) in flow]\n",
    "solution['word 2'] = [j for (i,j) in flow]\n",
    "solution['flow'] = [flow[i,j] for (i,j) in flow]\n",
    "solution['distance'] = [distance[i,j] for (i,j) in flow] \n",
    "# solution.sort_values(by='flow',ascending=False).reset_index(drop=True)\n",
    "solution.sort_values(by='distance',ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b95aebd",
   "metadata": {},
   "source": [
    "## IV. 检测抄袭"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d52912",
   "metadata": {},
   "source": [
    "在本notebook的这一部分中,我们检查一段给定的文本是否是从一本书中重写(或抄袭)的。为此,我们将给定的文本与书中的每个句子进行比较,并输出差异度最小的句子。之后,人工可以对这是否确实是抄袭案例做出最终评估。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d72ffb9",
   "metadata": {},
   "source": [
    "首先,将这本书(《福尔摩斯探案集》)作为文本文件读入。我们从[Project Gutenberg](https://www.gutenberg.org/cache/epub/1661/pg1661.txt)下载了这个文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a57b0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import requests\n",
    "\n",
    "master = \"https://raw.githubusercontent.com/Gurobi/modeling-examples/master/text_dissimilarity/PG1661_raw.txt\"\n",
    "content = requests.get(master)\n",
    "content = content.text\n",
    "content = content.replace('\\n',' ')\n",
    "content = content.replace('_',' ')\n",
    "content = content.replace('\\r','') \n",
    "sentences = list(map(str.strip, content.split(\".\")))[19:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66b4bc",
   "metadata": {},
   "source": [
    "对书中的所有句子进行预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f050e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(document):\n",
    "    # 去掉专有名词\n",
    "    tagged_doc = nltk.tag.pos_tag(document.split())\n",
    "    edited_sentence = [word for word,tag in tagged_doc]  \n",
    "    edited_sentence = [word for word,tag in tagged_doc if tag not in ['NNP','NNPS']]  \n",
    "\n",
    "    # 删除标点符号\n",
    "    tokenizer = RegexpTokenizer(r'\\w+') \n",
    "    processed_doc = tokenizer.tokenize(' '.join(edited_sentence)) \n",
    "\n",
    "    # 删除停止词\n",
    "    processed_doc = [i for i in processed_doc if i not in stopwords.words('english')]  \n",
    "    \n",
    "    return processed_doc\n",
    "\n",
    "processed_sentences = [] # 所有句子列表\n",
    "for s in sentences: \n",
    "    processed_sentences.append(pre_processing(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694e3c5",
   "metadata": {},
   "source": [
    "我们可以将WMD优化模型写在一个函数中,该函数输入两个文档,输出它们的差异度分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c15a1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_dissimilarity(D1, D2):\n",
    "\n",
    "    D1 = set(D1)\n",
    "    D2 = set(D2) \n",
    "    D2 = D2 - set([i for i in D2 if i not in model]) # 如果书中的某些单词不在word2vec数据中\n",
    "    D1 = D1 - set([i for i in D1 if i not in model])\n",
    "            \n",
    "    freqency_D1 = {i: list(D1).count(i)/len(D1) for i in D1}\n",
    "    freqency_D2 = {i: list(D2).count(i)/len(D2) for i in D2}\n",
    "    \n",
    "    if len(D2) < 5: # 如果句子太小，我们设置高不相似度，有效地忽略它\n",
    "        return 1\n",
    "        \n",
    "    m = gp.Model(\"Text_similarity\")\n",
    "    distance = {(i,j): spatial.distance.cosine(model[i],model[j]) for i in D1 for j in D2} \n",
    "\n",
    "    # 变量。调整这里的边界\n",
    "    f = m.addVars(D1,D2,name=\"f\",lb=0,ub=1) \n",
    "\n",
    "    # 最小化\n",
    "    m.ModelSense = GRB.MINIMIZE\n",
    "    m.setObjective(sum(f[w,w_prime]*distance[w,w_prime] for w in D1 for w_prime in D2))\n",
    "\n",
    "    # 添加约束\n",
    "    m.addConstrs(f.sum(w, '*') ==  freqency_D1[w] for w in D1)\n",
    "    m.addConstrs(f.sum('*', w_prime) == freqency_D2[w_prime] for w_prime in D2) \n",
    "    \n",
    "    m.setParam('OutputFlag', 0)\n",
    "    m.optimize()  \n",
    "    \n",
    "    return m.ObjVal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ebfb64",
   "metadata": {},
   "source": [
    "现在,我们选择一段从书中重写的段落。下面的代码有一些示例,但你也可以从[这本书](https://www.gutenberg.org/cache/epub/1661/pg1661.txt)中选择任何句子并创建你自己的抄袭版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a4de501",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "sample_sentence = 'With a gaze that shifted back and forth between us, the dimunitive figure appeared to be a mixture of apprehension and anticipation, uncertain if he was on the cusp of a fortune or a disaster.'\n",
    "# sample_sentence = 'Without much conversation, yet with a friendly gesture, he gestured towards an armchair for me to sit in, offered me a box of cigars, and pointed to a liquor cabinet and a carbonated water dispenser in the corner.' \n",
    "\n",
    "print(sample_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f010759",
   "metadata": {},
   "source": [
    "最后,我们遍历书中的所有句子,并找出差异度分数。下面的代码只在差异度降低时打印输出。结果是差异度最小的句子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "24114694",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "obj_best = 1\n",
    "print(\"#\\t Dissimilarity\\t Sentence\")\n",
    "for i in range(len(processed_sentences)):  \n",
    "    obj = score_dissimilarity(pre_processing(sample_sentence),processed_sentences[i])  \n",
    "    if obj < obj_best: \n",
    "        print(i,\"\\t\",round((obj),2),\"\\t\",sentences[i])\n",
    "        obj_best, sentence_best = obj, sentences[i]\n",
    "\n",
    "print(\"\\nThe closest sentence with a %f dissimilarity is:\\n\\n\"%obj_best,sentence_best) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8deb0c",
   "metadata": {},
   "source": [
    "太好了!程序是否正确识别了段落?试试其他段落及其重写版本吧!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b9df93",
   "metadata": {},
   "source": [
    "Copyright © 2023 Gurobi Optimization, LLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8632262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
